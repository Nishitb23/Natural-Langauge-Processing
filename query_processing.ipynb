{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "query_processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-IrRF5rnzQP",
        "outputId": "5fd94dca-51c3-403d-cd6c-264f1b1c0ade"
      },
      "source": [
        "!pip install contractions\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "import numpy as np\n",
        "from gensim.models import LsiModel\n",
        "from gensim import models\n",
        "from gensim import corpora\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Eo6fXGS2rBF",
        "outputId": "d8a7067d-a1f9-48f1-9ee9-7f24d97ad7ac"
      },
      "source": [
        "#loading the corpus\n",
        "sents = nltk.Text(brown.sents('cn12'))\n",
        "print(sents[0])\n",
        "print(\"No. of Documents: \",len(sents))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'several', 'minutes', 'had', 'passed', 'and', 'Curt', \"hadn't\", 'emerged', 'from', 'the', 'livery', 'stable', ',', 'Brenner', 'reentered', 'the', 'hotel', 'and', 'faced', 'Summers', 'across', 'the', 'counter', '.']\n",
            "No. of Documents:  186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ORMb9QF47E1"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN_3jb6Y4_Da"
      },
      "source": [
        "#remove contraction and stop words\n",
        "def removeContraction(sents_list):\n",
        "  new_sents = []\n",
        "  for s in sents_list:\n",
        "    words = []\n",
        "    for word in s:\n",
        "      word = contractions.fix(word)\n",
        "      word_split = word.split(' ')\n",
        "      for w in word_split:\n",
        "        if not w.lower() in stop_words:\n",
        "          words.append(w.lower())\n",
        "    if len(words) != 0:\n",
        "      new_sents.append(words)\n",
        "\n",
        "  return new_sents"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CORLzy05kxM",
        "outputId": "41a3bf83-0b87-4405-ac40-68cdd28eb7fc"
      },
      "source": [
        "sents = removeContraction(sents) #removing contraction for eg. I'll -> I will \n",
        "\n",
        "#pre-processing\n",
        "modified_sents = []\n",
        "for sent in sents:\n",
        "  words = []\n",
        "  for word in sent:\n",
        "       word = ps.stem(word) #performing stemming for e.g several -> sever\n",
        "       word = lemmatizer.lemmatize(word, pos=\"v\") #performing lemmitization for eg. running -> run\n",
        "       if word.isalpha():\n",
        "        words.append(word)\n",
        "  if len(words) != 0:\n",
        "    modified_sents.append(words)\n",
        "\n",
        "print(modified_sents[0])\n",
        "print(len(modified_sents))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sever', 'minut', 'pass', 'curt', 'emerg', 'liveri', 'stabl', 'brenner', 'reenter', 'hotel', 'face', 'summer', 'across', 'counter']\n",
            "180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL3xO2WH5og6",
        "outputId": "629e7e9d-80bc-44c0-923f-1866b58dad6c"
      },
      "source": [
        "#getting the vocabulary\n",
        "vocab = corpora.Dictionary(modified_sents)\n",
        "print(vocab)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(480 unique tokens: ['across', 'brenner', 'counter', 'curt', 'emerg']...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qYqAroly845"
      },
      "source": [
        "doc_term_matrix = [vocab.doc2bow(tokens) for tokens in modified_sents] #term frequency document wise\n",
        "\n",
        "tfidf = models.TfidfModel(doc_term_matrix) \n",
        "corpus_tfidf = tfidf[doc_term_matrix] #obtaining tfidf from term frequency matrix\n",
        "model = models.LsiModel(corpus_tfidf, id2word=vocab, num_topics=50) #reducing the dimensionality from 480 to 50"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLlistj20y0c"
      },
      "source": [
        "query = \"give messag dian molinari\"\n",
        "query_vector = vocab.doc2bow(query.lower().split()) #converting query into vector"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3oUi8Ji8vzC"
      },
      "source": [
        "query_vector_ld = model[query_vector]\n",
        "index = MatrixSimilarity(model[corpus_tfidf])\n",
        "sim_doc = index[query_vector_ld] #obtaining the similarity of each document with the query\n",
        "sim_doc = sorted(enumerate(sim_doc), key=lambda item: -item[1])[0:10] #sorting the similarity"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CwrzznY-HrN",
        "outputId": "4111901f-9054-4572-d8a3-58be8e7223a7"
      },
      "source": [
        "for i,similarity in sim_doc:\n",
        "  print(similarity,modified_sents[i]) #display top 10 similar document"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8035782 ['want', 'take', 'messag', 'dian', 'molinari']\n",
            "0.7777617 ['long', 'ride', 'four', 'take', 'must', 'give', 'good', 'appetit']\n",
            "0.65213066 ['appar', 'sens', 'realiz', 'give', 'advantag', 'jess', 'becam', 'bold']\n",
            "0.63398355 ['give', 'curt', 'time', 'stagger', 'feet']\n",
            "0.4056505 ['find', 'weak', 'jess', 'weak', 'smart', 'enough', 'take', 'advantag']\n",
            "0.4012061 ['lurch', 'drunkenli', 'feet', 'lower', 'head', 'take', 'one', 'step', 'away', 'wall']\n",
            "0.34306452 ['somebodi', 'town', 'must', 'still', 'backbon']\n",
            "0.33088678 ['want', 'brenner']\n",
            "0.31547165 ['curt', 'want', 'get', 'jess', 'alon', 'without', 'interfer', 'anyon', 'even', 'spineless', 'person', 'store', 'owner']\n",
            "0.27620968 ['want', 'hear', 'say']\n"
          ]
        }
      ]
    }
  ]
}