# -*- coding: utf-8 -*-
"""NLP_Assign1 Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oEt9E_CPXGvdx1GPRVn97YA7uDEi-L6

# **Module 1**
"""

import os
import json
import gzip
import pandas as pd
from urllib.request import urlopen
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import string
import re
from bs4 import BeautifulSoup
!pip install contractions
!pip install nltk
import nltk
import contractions
from collections import Counter
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import HashingVectorizer

#readin the data from file and generate dataframe
def parse(path):
  g = gzip.open(path, 'rb')
  for l in g:
    yield eval(l)

def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')

df = getDF('reviews_Amazon_Instant_Video_5.json.gz')
print(df['reviewText'][0:10])

reviewText = np.array(df.reviewText)
print(reviewText[0:10])
ratings = np.array(df.overall)
print(ratings[0:10])

#converting rating to good('1') and bad('0')
for i in range(len(ratings)):
  if ratings[i]>3:
    ratings[i] = 1
  else:
    ratings[i] = 0  
print(ratings[0:10])

nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
def preprocess_string(s):
  # Remove all non-word characters (everything except numbers and letters)
  s = re.sub(r"[^\w\s]", '', s)
  # Replace all runs of whitespaces with no space
  s = re.sub(r"\s+", '', s)
  # replace digits with no space
  s = re.sub(r"\d", '', s)
  #lemmatization
  return lemmatizer.lemmatize(s)

def replace_contactions(text):
  return contractions.fix(text)

#building the vocabulary
word_list = []
review_array = []
for sentence in reviewText:
  sentence = replace_contactions(sentence)
  sentence_array = []
  for word in sentence.lower().split():
    word = preprocess_string(word)
    # print(word)
    sentence_array.append(word)
    word_list.append(word)
  review_array.append(sentence_array)
corpus = Counter(word_list)
print(len(corpus))
print(review_array[0])

print(corpus)

from nltk.util import ngrams
n = 2
n_grams = ngrams(word_list, n)
n_corpus = Counter(n_grams)
print(n_corpus.most_common()[0:100])

n = 3
n_grams = ngrams(word_list, n)
n_corpus = Counter(n_grams)
print(n_corpus.most_common()[0:100])

frequency = np.array(sorted(corpus.values(),reverse=True))
x_axis = np.array(range(1,len(frequency)+1))
print("frequency: ",frequency)
print("rank: ",x_axis)
frequency = np.log(frequency)
print("frequency in log: ",frequency)
x_axis = np.log(x_axis)
print("rank in log: ",x_axis)

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(x_axis.reshape(-1,1), frequency.reshape(-1,1))
pred_list = []
y_axis = reg.predict(x_axis.reshape(-1,1))
print(y_axis)
#plt.figure(figsize=(16, 9))
plt.figure()
plt.plot(x_axis,frequency,label='frequency')
plt.plot(x_axis,y_axis,label='best fit')
plt.xlabel('Log( rank )')
plt.ylabel('Log( frequency )')
plt.title("Log(frequency) vs log(rank)")
legend = []
legend.append("Log(frequency)")
legend.append("Regression Curve")
plt.legend(legend, loc='upper right')
plt.show()

#extracting the useful words from the corpus and removing others
corpus = corpus.most_common()[150:15000]
print(corpus)

corpus = dict(corpus)
print(corpus)

new_list = []
for i in range(len(review_array)):
  str1 = ""
  for word in review_array[i]:
    if corpus.get(word):
      str1 = str1 + word+ " "
  new_list.append(str1)
review_array = new_list
print(review_array[1])

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
lower_case = reviewText[1].lower()
tokens = nltk.word_tokenize(lower_case)
print(lower_case)
print(nltk.pos_tag(tokens))

"""# **Module 2**"""

#CountVectorization

def count_vectorization(text):
  # Create a Vectorizer Object
  vectorizer = CountVectorizer()

  # text = reviewText[0:10]
  # print(text)
 
  vectorizer.fit(text)
  # Printing the identified Unique words along with their indices
  print("Vocabulary: ", vectorizer.vocabulary_)

  # Encode the Document
  vector = vectorizer.transform(text)
  
  # Summarizing the Encoded Texts
  print("Encoded Reviews is:")
  enc = vector.toarray()
  # type(enc)
  print(vector.toarray())
  return vector

#TF-IDF
def cal_tfidf(text):
  from sklearn.feature_extraction.text import TfidfVectorizer
  # create the transform
  vectorizer = TfidfVectorizer()
  # tokenize and build vocab
  vectorizer.fit(text)
  # summarize
  print(vectorizer.vocabulary_)
  print(vectorizer.idf_)
  # encode document
  vector = vectorizer.transform(text)
  # summarize encoded vector
  # print(vector.shape)
  # print(vector.toarray())
  return vector

#Hash Vecotrization
def hashvec(text):
  vectorizer  = HashingVectorizer(n_features=20)
  vector = vectorizer.transform(text)
  return vector

#confusion matrix
def conf_matrix(y_test,y_pred):
  return ((confusion_matrix(y_test, y_pred)))

#Accuracy calculations
def cal_acc(y_test,y_pred):
  accuracy = accuracy_score(y_test, y_pred)
  return accuracy

#f1 Score
def cal_f1_score(y_test,y_pred):
  score = f1_score(y_test, y_pred, average='binary')
  return score

#Decision Tree
def decisiontree(X_train,y_train ,X_test, y_test):
  from sklearn.tree import DecisionTreeClassifier
  classifier = DecisionTreeClassifier()
  classifier.fit(X_train, y_train)

  y_pred = classifier.predict(X_test)
  
  print("confusion matrix: \n",conf_matrix(y_test, y_pred))
  print("accuracy: ",cal_acc(y_test, y_pred))
  print("f1 score: ",cal_f1_score(y_test, y_pred))


# Logistic Regression
def logisticregression(X_train, y_train , X_test, y_test):

  from sklearn.linear_model import LogisticRegression

  logistic_regression = LogisticRegression(max_iter= 500)
  logistic_regression.fit(X_train,y_train)
  y_pred = logistic_regression.predict(X_test)
  print("confusion matrix: \n",conf_matrix(y_test, y_pred))
  print("accuracy: ",cal_acc(y_test, y_pred))
  print("f1 score: ",cal_f1_score(y_test, y_pred))

def naivebayes(X_train,y_train ,X_test, y_test):
  from sklearn.naive_bayes import GaussianNB
  x = X_train.toarray()
  X_test = X_test.toarray()
  GausNB = GaussianNB()
  GausNB.fit(x, y_train)
  y_pred = GausNB.predict(X_test)
  print("confusion matrix: \n",conf_matrix(y_test, y_pred))
  print("accuracy: ",cal_acc(y_test, y_pred))
  print("f1 score: ",cal_f1_score(y_test, y_pred))

vect = [count_vectorization ,cal_tfidf, hashvec]
classifier = [decisiontree , logisticregression,naivebayes]

for v in vect:
  y = ratings
  X = v(review_array)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)
  for c in classifier:
    print("Results for",v.__name__ ,"and classifier",c.__name__ )
    c(X_train , y_train ,X_test, y_test)
    print()

"""# **Module 3**"""

from sklearn.decomposition import LatentDirichletAllocation

#getting data vectors using count vectorization
data_vectors = count_vectorization(review_array)

n_topic = [1,2,3,4,5]

for n in n_topic:
  LDA = LatentDirichletAllocation(n_components=n, random_state=42)
  LDA.fit(data_vectors)
  print("perplexity for n=",n,":", LDA.perplexity(data_vectors))

vectorizer = CountVectorizer()
vectorizer.fit(review_array)
data_vectors = vectorizer.transform(review_array)

LDA = LatentDirichletAllocation(n_components=3, random_state=42)
LDA.fit(data_vectors)

for i,topic in enumerate(LDA.components_):
    print(f'words for topic #{i}:')
    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-200:]])
    print('\n')

topic_values = LDA.transform(data_vectors)
print(topic_values.shape)
print(topic_values)

topic_column = topic_values.argmax(axis=1)
print(topic_column)

good_review_list = []
bad_review_list = []

#topic 0 analysis
bad_count = 0
good_count = 0
sentence_list = []
for i in range(len(topic_column)):
  if topic_column[i] == 0:
    #print("Sentence under topic 0 : ",reviewText[i])
    sentence_list.append(reviewText[i])
    if ratings[i] == 0:
      bad_count += 1
    else:
      good_count += 1
good_review_list.append(good_count)
bad_review_list.append(bad_count)
print("sentences(count = 10) under topic 0\n",sentence_list[0:10])

#topic 1 analysis
bad_count = 0
good_count = 0
sentence_list = []
for i in range(len(topic_column)):
  if topic_column[i] == 1:
    #print("Sentence under topic 1 : ",reviewText[i])
    sentence_list.append(reviewText[i])
    if ratings[i] == 0:
      bad_count += 1
    else:
      good_count += 1
good_review_list.append(good_count)
bad_review_list.append(bad_count)
print("sentences(count = 10) under topic 1\n",sentence_list[0:10])

#topic 2 analysis
bad_count = 0
good_count = 0
sentence_list = []
for i in range(len(topic_column)):
  if topic_column[i] == 2:
    #print("Sentence under topic 2 : ",reviewText[i])\
    sentence_list.append(reviewText[i])
    if ratings[i] == 0:
      bad_count += 1
    else:
      good_count += 1
good_review_list.append(good_count)
bad_review_list.append(bad_count)
print("sentences(count = 10) under topic 2\n",sentence_list[0:10])

print("topic-wise good review count:",good_review_list)
print("topic-wise bad review count:",bad_review_list)

print("percentage bad to good review topic wise:\n")
for i in range(3):
  print("topic",i,(bad_review_list[i]/good_review_list[i])*100,"%")