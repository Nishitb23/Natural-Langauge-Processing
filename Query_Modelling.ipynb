{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Query_Modelling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucVIrv4wfmGD",
        "outputId": "4f1d29f4-a813-4cad-c311-99783c00e83b"
      },
      "source": [
        "!pip install contractions\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import contractions\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXq3QN30hRyW",
        "outputId": "d1518b62-a01b-4619-9fad-baf25c34fcf5"
      },
      "source": [
        "#fetching the documents in the cn12 file in the corpus\n",
        "sents = nltk.Text(brown.sents('cn12'))\n",
        "print(sents[0])\n",
        "print(len(sents))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'several', 'minutes', 'had', 'passed', 'and', 'Curt', \"hadn't\", 'emerged', 'from', 'the', 'livery', 'stable', ',', 'Brenner', 'reentered', 'the', 'hotel', 'and', 'faced', 'Summers', 'across', 'the', 'counter', '.']\n",
            "186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MgZ42iM7a0D"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofI2uXK9UX0"
      },
      "source": [
        "def removeContraction(sents_list):\n",
        "  new_sents = []\n",
        "  for s in sents_list:\n",
        "    words = []\n",
        "    for word in s:\n",
        "      word = contractions.fix(word)\n",
        "      word_split = word.split(' ')\n",
        "      for w in word_split:\n",
        "        words.append(w.lower())\n",
        "    new_sents.append(words)\n",
        "\n",
        "  return new_sents\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9IZpePlIj4Q",
        "outputId": "4d4e3197-2265-468b-f48e-899060063fc8"
      },
      "source": [
        "sents = removeContraction(sents) #removing contraction for eg. I'll -> I will \n",
        "\n",
        "#pre-processing\n",
        "modified_sents = []\n",
        "for sent in sents:\n",
        "  words = []\n",
        "  for word in sent:\n",
        "       word = ps.stem(word) #performing stemming for e.g several -> sever\n",
        "       word = lemmatizer.lemmatize(word, pos=\"v\") #performing lemmitization for eg. running -> run\n",
        "       if word.isalpha():\n",
        "        words.append(word)\n",
        "  modified_sents.append(words)\n",
        "\n",
        "print(modified_sents[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when', 'sever', 'minut', 'have', 'pass', 'and', 'curt', 'have', 'not', 'emerg', 'from', 'the', 'liveri', 'stabl', 'brenner', 'reenter', 'the', 'hotel', 'and', 'face', 'summer', 'across', 'the', 'counter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP-CeGe8_i5Y",
        "outputId": "75d8a838-0e0e-4afe-ef26-6af55e225468"
      },
      "source": [
        "#creating vocabulary \n",
        "vocab = []\n",
        "\n",
        "for sent in modified_sents:\n",
        "  for word in sent:\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "\n",
        "print(len(vocab))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx1wIVVhe0Yu"
      },
      "source": [
        "#creating map of word and position in vocab\n",
        "posmap = {}\n",
        "for i,word in enumerate(vocab):\n",
        "  posmap[word] = i\n",
        "\n",
        "#print(posmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ey5-P1igFhb",
        "outputId": "d0dab21d-9a81-477f-d9bd-fcc0b54c0f38"
      },
      "source": [
        "#creating incidence matrix\n",
        "mat = np.zeros((len(modified_sents),len(vocab)))\n",
        "print(mat.shape)\n",
        "\n",
        "for s_index,sent in enumerate(modified_sents):\n",
        "  for word in sent:\n",
        "    mat[s_index][posmap[word]] += 1\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(186, 561)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWcZtHxNkHn-"
      },
      "source": [
        "#calculating document frequency\n",
        "doc_freq = np.zeros(len(vocab))\n",
        "\n",
        "for col in range(mat.shape[1]):\n",
        "  for row in range(mat.shape[0]):\n",
        "    if mat[row][col] !=0:\n",
        "      doc_freq[col] +=1"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq4FLDaGsnz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4fab66-e43c-452d-8bfc-42943de569ec"
      },
      "source": [
        "#calculating inverse document frquency\n",
        "idf = []\n",
        "for df in doc_freq:\n",
        "  idf.append(math.log(len(sents)/df,10))\n",
        "\n",
        "idf = np.array(idf)\n",
        "print(idf.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(561,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExkmO9n4-DFL"
      },
      "source": [
        "Weighted TF-IDF: (1+log(tf))*(log(N/df))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwz_pVVswWU"
      },
      "source": [
        "#calculating weighted tf-idf\n",
        "for col in range(mat.shape[1]):\n",
        "  for row in range(mat.shape[0]):\n",
        "    if mat[row][col] !=0:\n",
        "      mat[row][col] = (1+math.log(mat[row][col],10))*idf[col]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvR5JlDYtXbN"
      },
      "source": [
        "#defining query\n",
        "query = ['give','messag','to','dian','molinari']\n",
        "\n",
        "#defining query vector\n",
        "query_vector = np.zeros(len(vocab))\n",
        "for word in query:\n",
        "  query_vector[posmap[word]] +=1\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxl2hsyb6188"
      },
      "source": [
        "#finding minimun cosine distance to find most similar document\n",
        "min_distance = 1.1\n",
        "min_index = 0\n",
        "for index in range(mat.shape[0]):\n",
        "  d = distance.cosine(query_vector,mat[index])\n",
        "  if d<min_distance:\n",
        "    min_distance = d\n",
        "    min_index = index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJyR4quZ9kTv",
        "outputId": "b5a103a8-4346-4e0e-cfb1-e3b62db7f1d3"
      },
      "source": [
        "#printing the similar document \n",
        "print(modified_sents[min_index])\n",
        "print(query)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'just', 'want', 'you', 'to', 'take', 'a', 'messag', 'to', 'dian', 'molinari']\n",
            "['give', 'messag', 'to', 'dian', 'molinari']\n"
          ]
        }
      ]
    }
  ]
}