{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Query_Modelling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucVIrv4wfmGD",
        "outputId": "4f1d29f4-a813-4cad-c311-99783c00e83b"
      },
      "source": [
        "!pip install contractions\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import contractions\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXq3QN30hRyW",
        "outputId": "d1518b62-a01b-4619-9fad-baf25c34fcf5"
      },
      "source": [
        "#fetching the documents in the cn12 file in the corpus\n",
        "sents = nltk.Text(brown.sents('cn12'))\n",
        "print(sents[0])\n",
        "print(len(sents))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'several', 'minutes', 'had', 'passed', 'and', 'Curt', \"hadn't\", 'emerged', 'from', 'the', 'livery', 'stable', ',', 'Brenner', 'reentered', 'the', 'hotel', 'and', 'faced', 'Summers', 'across', 'the', 'counter', '.']\n",
            "186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MgZ42iM7a0D"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofI2uXK9UX0"
      },
      "source": [
        "def removeContraction(sents_list):\n",
        "  new_sents = []\n",
        "  for s in sents_list:\n",
        "    words = []\n",
        "    for word in s:\n",
        "      word = contractions.fix(word)\n",
        "      word_split = word.split(' ')\n",
        "      for w in word_split:\n",
        "        words.append(w.lower())\n",
        "    new_sents.append(words)\n",
        "\n",
        "  return new_sents\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9IZpePlIj4Q",
        "outputId": "4d4e3197-2265-468b-f48e-899060063fc8"
      },
      "source": [
        "sents = removeContraction(sents) #removing contraction for eg. I'll -> I will \n",
        "\n",
        "#pre-processing\n",
        "modified_sents = []\n",
        "for sent in sents:\n",
        "  words = []\n",
        "  for word in sent:\n",
        "       word = ps.stem(word) #performing stemming for e.g several -> sever\n",
        "       word = lemmatizer.lemmatize(word, pos=\"v\") #performing lemmitization for eg. running -> run\n",
        "       if word.isalpha():\n",
        "        words.append(word)\n",
        "  modified_sents.append(words)\n",
        "\n",
        "print(modified_sents[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when', 'sever', 'minut', 'have', 'pass', 'and', 'curt', 'have', 'not', 'emerg', 'from', 'the', 'liveri', 'stabl', 'brenner', 'reenter', 'the', 'hotel', 'and', 'face', 'summer', 'across', 'the', 'counter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP-CeGe8_i5Y",
        "outputId": "75d8a838-0e0e-4afe-ef26-6af55e225468"
      },
      "source": [
        "#creating vocabulary \n",
        "vocab = []\n",
        "\n",
        "for sent in modified_sents:\n",
        "  for word in sent:\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "\n",
        "print(len(vocab))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx1wIVVhe0Yu",
        "outputId": "16410579-cda6-409b-ab43-f3b639b8650f"
      },
      "source": [
        "#creating map of word and position in vocab\n",
        "posmap = {}\n",
        "for i,word in enumerate(vocab):\n",
        "  posmap[word] = i\n",
        "\n",
        "print(posmap)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'when': 0, 'sever': 1, 'minut': 2, 'have': 3, 'pass': 4, 'and': 5, 'curt': 6, 'not': 7, 'emerg': 8, 'from': 9, 'the': 10, 'liveri': 11, 'stabl': 12, 'brenner': 13, 'reenter': 14, 'hotel': 15, 'face': 16, 'summer': 17, 'across': 18, 'counter': 19, 'i': 20, 'a': 21, 'littl': 22, 'job': 23, 'for': 24, 'you': 25, 'charli': 26, 'be': 27, 'sure': 28, 'will': 29, 'mind': 30, 'do': 31, 'me': 32, 'small': 33, 'favor': 34, 'voic': 35, 'wa': 36, 'oili': 37, 'but': 38, 'fool': 39, 'he': 40, 'moisten': 41, 'hi': 42, 'lip': 43, 'uneasili': 44, 'what': 45, 'it': 46, 'want': 47, 'to': 48, 'shrug': 49, 'carelessli': 50, 'veri': 51, 'simpl': 52, 'just': 53, 'take': 54, 'messag': 55, 'dian': 56, 'molinari': 57, 'tell': 58, 'her': 59, 'come': 60, 'here': 61, 'vastli': 62, 'reliev': 63, 'nod': 64, 'start': 65, 'toward': 66, 'door': 67, 'one': 68, 'thing': 69, 'say': 70, 'mention': 71, 'my': 72, 'name': 73, 'adam': 74, 'see': 75, 'pull': 76, 'up': 77, 'short': 78, 'turn': 79, 'around': 80, 'know': 81, 'haltingli': 82, 'begin': 83, 'get': 84, 'an': 85, 'inkl': 86, 'of': 87, 'plan': 88, 'doe': 89, 'seem': 90, 'quit': 91, 'right': 92, 'like': 93, 'that': 94, 'could': 95, 'trail': 96, 'off': 97, 'into': 98, 'silenc': 99, 'continu': 100, 'smile': 101, 'eye': 102, 'cold': 103, 'look': 104, 'at': 105, 'lobbi': 106, 'as': 107, 'though': 108, 'befor': 109, 'notic': 110, 'think': 111, 'nest': 112, 'ought': 113, 'fire': 114, 'compani': 115, 'if': 116, 'someon': 117, 'drop': 118, 'match': 119, 'in': 120, 'thi': 121, 'place': 122, 'would': 123, 'go': 124, 'haystack': 125, 'stairway': 126, 'then': 127, 'add': 128, 'room': 129, 'troubl': 130, 'hurri': 131, 'ye': 132, 'sir': 133, 'pale': 134, 'head': 135, 'street': 136, 'visit': 137, 'mere': 138, 'precaut': 139, 'case': 140, 'anyon': 141, 'should': 142, 'watch': 143, 'paus': 144, 'onli': 145, 'long': 146, 'enough': 147, 'ascertain': 148, 'buckskin': 149, 'still': 150, 'miss': 151, 'own': 152, 'gray': 153, 'all': 154, 'climb': 155, 'through': 156, 'back': 157, 'window': 158, 'grind': 159, 'outsid': 160, 'fact': 161, 'hors': 162, 'return': 163, 'stall': 164, 'indic': 165, 'inform': 166, 'wrong': 167, 'interpret': 168, 'way': 169, 'man': 170, 'jess': 171, 'readi': 172, 'mean': 173, 'escap': 174, 'need': 175, 'probabl': 176, 'close': 177, 'where': 178, 'hide': 179, 'barn': 180, 'matter': 181, 'reach': 182, 'hous': 183, 'without': 184, 'use': 185, 'approach': 186, 'cautious': 187, 'protect': 188, 'grove': 189, 'tree': 190, 'there': 191, 'light': 192, 'front': 193, 'draw': 194, 'curtain': 195, 'prevent': 196, 'ani': 197, 'view': 198, 'interior': 199, 'circl': 200, 'locat': 201, 'out': 202, 'hear': 203, 'move': 204, 'insid': 205, 'noth': 206, 'els': 207, 'no': 208, 'lock': 209, 'on': 210, 'iron': 211, 'hook': 212, 'which': 213, 'unfasten': 214, 'open': 215, 'shut': 216, 'behind': 217, 'him': 218, 'again': 219, 'stand': 220, 'dark': 221, 'listen': 222, 'scrape': 223, 'shoe': 224, 'hoof': 225, 'plank': 226, 'floor': 227, 'ahead': 228, 'care': 229, 'leave': 230, 'hand': 231, 'wooden': 232, 'partit': 233, 'smell': 234, 'strong': 235, 'crunch': 236, 'grain': 237, 'between': 238, 'jaw': 239, 'find': 240, 'pocket': 241, 'two': 242, 'dun': 243, 'snuf': 244, 'certain': 245, 'now': 246, 'also': 247, 'presum': 248, 'stacey': 249, 'black': 250, 'alon': 251, 'interfer': 252, 'even': 253, 'spineless': 254, 'person': 255, 'store': 256, 'owner': 257, 'studi': 258, 'problem': 259, 'few': 260, 'second': 261, 'by': 262, 'might': 263, 'solv': 264, 'side': 265, 'slap': 266, 'rump': 267, 'startl': 268, 'anim': 269, 'let': 270, 'terrifi': 271, 'squeal': 272, 'thrash': 273, 'hope': 274, 'bang': 275, 'kick': 276, 'wildli': 277, 'rattl': 278, 'over': 279, 'besid': 280, 'wait': 281, 'present': 282, 'footstep': 283, 'cross': 284, 'yard': 285, 'smother': 286, 'curs': 287, 'swing': 288, 'sourli': 289, 'with': 290, 'snort': 291, 'doubt': 292, 'belong': 293, 'much': 294, 'reassur': 295, 'enter': 296, 'flare': 297, 'abov': 298, 'lantern': 299, 'hang': 300, 'wire': 301, 'loop': 302, 'gun': 303, 'click': 304, 'hammer': 305, 'tri': 306, 'anyth': 307, 'rememb': 308, 'happen': 309, 'gruller': 310, 'catch': 311, 'breath': 312, 'surpris': 313, 'appar': 314, 'better': 315, 'stuff': 316, 'hold': 317, 'fasten': 318, 'slide': 319, 'bolt': 320, 'talk': 321, 'lift': 322, 'pistol': 323, 'holster': 324, 'damn': 325, 'recov': 326, 'initi': 327, 'shock': 328, 'we': 329, 'about': 330, 'your': 331, 'brother': 332, 'supper': 333, 'ride': 334, 'four': 335, 'must': 336, 'give': 337, 'good': 338, 'appetit': 339, 'so': 340, 'can': 341, 'raw': 342, 'furi': 343, 'vein': 344, 'neck': 345, 'swell': 346, 'dumb': 347, 'they': 348, 'forget': 349, 'too': 350, 'ben': 351, 'arbuckl': 352, 'stiffen': 353, 'suppos': 354, 'piec': 355, 'either': 356, 'blood': 357, 'finger': 358, 'put': 359, 'more': 360, 'pressur': 361, 'trigger': 362, 'help': 363, 'crouch': 364, 'kill': 365, 'admit': 366, 'who': 367, 'dutch': 368, 'arm': 369, 'slash': 370, 'gunbarrel': 371, 'finish': 372, 'motion': 373, 'unarm': 374, 'easi': 375, 'bring': 376, 'himself': 377, 'sens': 378, 'realiz': 379, 'advantag': 380, 'becam': 381, 'bold': 382, 'make': 383, 'big': 384, 'both': 385, 'tough': 386, 'retreat': 387, 'step': 388, 'term': 389, 'sweat': 390, 'bubbl': 391, 'swarthi': 392, 'twist': 393, 'claw': 394, 'angri': 395, 'reckless': 396, 'rais': 397, 'shoulder': 398, 'high': 399, 'easier': 400, 'coward': 401, 'against': 402, 'thickli': 403, 'how': 404, 'outdraw': 405, 'chico': 406, 'gunsling': 407, 'kind': 408, 'bastard': 409, 'sneak': 410, 'hit': 411, 'club': 412, 'stare': 413, 'answer': 414, 'fall': 415, 'weak': 416, 'or': 417, 'smart': 418, 'somewher': 419, 'distanc': 420, 'woman': 421, 'scream': 422, 'involv': 423, 'pay': 424, 'attent': 425, 'curiou': 426, 'investig': 427, 'onc': 428, 'time': 429, 'toss': 430, 'frighten': 431, 'anoth': 432, 'harshli': 433, 'coars': 434, 'featur': 435, 'grin': 436, 'smash': 437, 'shape': 438, 'fist': 439, 'roar': 440, 'pain': 441, 'attack': 442, 'manag': 443, 'duck': 444, 'beneath': 445, 'flail': 446, 'drive': 447, 'home': 448, 'solid': 449, 'sack': 450, 'salt': 451, 'shoot': 452, 'clear': 453, 'hardli': 454, 'awar': 455, 'slam': 456, 'wall': 457, 'bounc': 458, 'roundhous': 459, 'send': 460, 'spin': 461, 'inch': 462, 'lower': 463, 'knock': 464, 'vision': 465, 'blur': 466, 'moment': 467, 'unabl': 468, 'focu': 469, 'saw': 470, 'charg': 471, 'pitchfork': 472, 'tine': 473, 'fork': 474, 'bite': 475, 'wast': 476, 'yank': 477, 'them': 478, 'loos': 479, 'stagger': 480, 'feet': 481, 'break': 482, 'under': 483, 'handl': 484, 'attempt': 485, 'brain': 486, 'aim': 487, 'whistl': 488, 'clearli': 489, 'guard': 490, 'land': 491, 'blow': 492, 'belli': 493, 'everyth': 494, 'doubl': 495, 'down': 496, 'grab': 497, 'hair': 498, 'catapult': 499, 'first': 500, 'build': 501, 'shake': 502, 'set': 503, 'sway': 504, 'pitch': 505, 'old': 506, 'crowbait': 507, 'yell': 508, 'lurch': 509, 'drunkenli': 510, 'away': 511, 'flush': 512, 'nose': 513, 'gush': 514, 'rapidli': 515, 'stumbl': 516, 'frantic': 517, 'hast': 518, 'almost': 519, 'bad': 520, 'corner': 521, 'shirtfront': 522, 'idea': 523, 'mumbl': 524, 'dab': 525, 'show': 526, 'town': 527, 'shirt': 528, 'slump': 529, 'unti': 530, 'halter': 531, 'rope': 532, 'wave': 533, 'pick': 534, 'straw': 535, 'action': 536, 'leav': 537, 'lay': 538, 'someplac': 539, 'call': 540, 'fear': 541, 'stay': 542, 'while': 543, 'dirti': 544, 'somebodi': 545, 'shove': 546, 'point': 547, 'along': 548, 'peac': 549, 'bullet': 550, 'leg': 551, 'follow': 552, 'delay': 553, 'ask': 554, 'worriedli': 555, 'marshal': 556, 'mayb': 557, 'differ': 558, 'some': 559, 'backbon': 560}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ey5-P1igFhb",
        "outputId": "d0dab21d-9a81-477f-d9bd-fcc0b54c0f38"
      },
      "source": [
        "#creating incidence matrix\n",
        "mat = np.zeros((len(modified_sents),len(vocab)))\n",
        "print(mat.shape)\n",
        "\n",
        "for s_index,sent in enumerate(modified_sents):\n",
        "  for word in sent:\n",
        "    mat[s_index][posmap[word]] += 1\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(186, 561)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWcZtHxNkHn-"
      },
      "source": [
        "#calculating document frequency\n",
        "doc_freq = np.zeros(len(vocab))\n",
        "\n",
        "for col in range(mat.shape[1]):\n",
        "  for row in range(mat.shape[0]):\n",
        "    if mat[row][col] !=0:\n",
        "      doc_freq[col] +=1"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq4FLDaGsnz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4fab66-e43c-452d-8bfc-42943de569ec"
      },
      "source": [
        "#calculating inverse document frquency\n",
        "idf = []\n",
        "for df in doc_freq:\n",
        "  idf.append(math.log(len(sents)/df,10))\n",
        "\n",
        "idf = np.array(idf)\n",
        "print(idf.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(561,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExkmO9n4-DFL"
      },
      "source": [
        "Weighted TF-IDF: (1+log(tf))*(log(N/df))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwz_pVVswWU"
      },
      "source": [
        "#calculating weighted tf-idf\n",
        "for col in range(mat.shape[1]):\n",
        "  for row in range(mat.shape[0]):\n",
        "    if mat[row][col] !=0:\n",
        "      mat[row][col] = (1+math.log(mat[row][col],10))*idf[col]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvR5JlDYtXbN"
      },
      "source": [
        "#defining query\n",
        "query = ['give','messag','to','dian','molinari']\n",
        "\n",
        "#defining query vector\n",
        "query_vector = np.zeros(len(vocab))\n",
        "for word in query:\n",
        "  query_vector[posmap[word]] +=1\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxl2hsyb6188"
      },
      "source": [
        "#finding minimun cosine distance to find most similar document\n",
        "min_distance = 1.1\n",
        "min_index = 0\n",
        "for index in range(mat.shape[0]):\n",
        "  d = distance.cosine(query_vector,mat[index])\n",
        "  if d<min_distance:\n",
        "    min_distance = d\n",
        "    min_index = index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJyR4quZ9kTv",
        "outputId": "b5a103a8-4346-4e0e-cfb1-e3b62db7f1d3"
      },
      "source": [
        "#printing the similar document \n",
        "print(modified_sents[min_index])\n",
        "print(query)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'just', 'want', 'you', 'to', 'take', 'a', 'messag', 'to', 'dian', 'molinari']\n",
            "['give', 'messag', 'to', 'dian', 'molinari']\n"
          ]
        }
      ]
    }
  ]
}